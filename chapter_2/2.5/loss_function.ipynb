{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19ff1fab-93ad-43f5-be49-2142eea4b9ca",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "The Boston house price prediction task in the first chapter is a regression task whose result is a continuous value, so we can use mean squared error as a loss function. However, the result of the classification task is discrete labeling, so it is unreasonable to use mean squared error as a loss function.\n",
    "\n",
    "## Softmax Function\n",
    "\n",
    "$$\n",
    "Softmax(x_i) = \\frac{e^{x_i}}{\\sum\\limits_{j=0}^{N}{e^{x_j}}}\n",
    "$$\n",
    "\n",
    "As can be seen from the formula, the range of each output is between 0~1, and the sum of all outputs is equal to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b71a9d-29f7-4dfe-a89b-d1ef2756e9b1",
   "metadata": {},
   "source": [
    "## Cross-entropy\n",
    "\n",
    "People are used to using cross-entropy as a loss function for classification problems.\n",
    "\n",
    "Firstly, \"one-bit valid encoding\" for categories:\n",
    "\n",
    "$$\n",
    "\\begin{array}\n",
    "\\mathbf{y} =& [y_1, y_2, \\dots, y_n]^T \\\\\n",
    "y_i =& \\left\\{ \\begin{array}\\\n",
    "    & 1\\ & \\mathrm{if}\\ i = y \\\\\n",
    "    & 0\\ & \\mathrm{otherwise}\n",
    "    \\end{array} \\right.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The maximum value is the forecast:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\mathop{\\mathrm{argmax}}\\limits_i o_i\n",
    "$$\n",
    "\n",
    "Cross-entropy is often used to measure the difference between two probabilities:\n",
    "\n",
    "$$\n",
    "H(\\mathbf{p},\\mathbf{q}) = -\\sum\\limits_i{p_i\\log{q_i}}\n",
    "$$\n",
    "\n",
    "Use it as loss function:\n",
    "\n",
    "$$\n",
    "l(y, \\hat{y}) = -\\sum\\limits_i y_i\\log{\\hat{y}_i} = -\\log{\\hat{y}_y}\n",
    "$$\n",
    "\n",
    "Its gradient is the difference between true probability and predicted probability:\n",
    "\n",
    "$$\n",
    "\\partial_{o_i} l(y, \\hat{y}) = \\mathrm{softmax}(\\mathbf{o})_i - y_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "32e5fd14-6af7-4723-bbbc-2ba26fca8fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddle.nn import Conv2D, MaxPool2D, Linear\n",
    "from PIL import Image\n",
    "import paddle.nn.functional as F\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "9dbe56db-58b9-4795-96c6-8f738086ab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_files_path = 'mnist.pdparams'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "ba70ca88-8f80-42b4-bac2-efeab4f0d53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(MNIST, self).__init__()\n",
    "\n",
    "        # Define the convolutional layer 1 with \n",
    "        # the output feature channel (out_channels) set to 20, \n",
    "        # the convolution kernel size (kernel_size) to 5, \n",
    "        # the convolution step size stride to 1, and padding to 2\n",
    "        self.conv1 = Conv2D(in_channels = 1, out_channels = 20, kernel_size = 5, stride = 1, padding = 2)\n",
    "        # Define the pooling layer 1 with  \n",
    "        # the size of the pooling kernel (kernel_size) to 2, \n",
    "        # the pooling step to 2\n",
    "        self.max_pool1 = MaxPool2D(kernel_size = 2, stride = 2)\n",
    "        # Define the convolutional layer 2\n",
    "        self.conv2 = Conv2D(in_channels = 20, out_channels = 20, kernel_size = 5, stride = 1, padding = 2)\n",
    "        # Define the pooling layer 2\n",
    "        self.max_pool2 = MaxPool2D(kernel_size = 2, stride = 2)\n",
    "        # Define a fully connected layer\n",
    "        self.fc = Linear(in_features = 980, out_features = 10)\n",
    "\n",
    "    # define farword calculation function, the activation function of hidden layers is ReLU\n",
    "    def forward(self, inputs, labels=None):\n",
    "        inputs = paddle.reshape(inputs, [inputs.shape[0], 1, 28, 28])\n",
    "        x = self.conv1(inputs)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool2(x)\n",
    "        x = paddle.reshape(x, [x.shape[0], -1])\n",
    "        x = self.fc(x)\n",
    "\n",
    "        # accuracy\n",
    "        # labels = paddle.unsqueeze(labels, axis = 1)\n",
    "        # x = paddle.to_tensor(images)\n",
    "        # labels = paddle.to_tensor(labels)\n",
    "        if labels is not None:\n",
    "            labels = paddle.reshape(labels, [labels.shape[0], 1])\n",
    "            acc = paddle.metric.accuracy(input = x, label = labels)\n",
    "            return x, acc\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "45c9e050-7682-4514-966c-5dc15c7dee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(mode = 'train', BATCH_SIZE = 100):\n",
    "    # Read Data and Divide Dataset\n",
    "    datafile = './work/mnist.json.gz'\n",
    "    print('loading mnist dataset from {} ......'.format(datafile))\n",
    "    ## load json data fiel\n",
    "    data = json.load(gzip.open(datafile))\n",
    "    print('mnist dataset load done')\n",
    "    ## unpackage the data into training set, verifying set and testing set.\n",
    "    train_set, val_set, eval_set = data\n",
    "    \n",
    "    # print(len(train_set[0][1]), train_set[0][1])\n",
    "    # print(len(train_set[0][3]), train_set[0][3])\n",
    "    # print(len(train_set[0][10000]), train_set[0][10000])\n",
    "    \n",
    "    if mode == 'train':\n",
    "        imgs, labels = train_set[0], train_set[1]\n",
    "    elif mode == 'valid':\n",
    "        imgs, labels = val_set[0], val_set[1]\n",
    "    elif mode == 'eval':\n",
    "        imgs, labels = eval_set[0], eval_set[1]\n",
    "    else:\n",
    "        raise Exception('mode can only be one of [\\'train\\', \\'valid\\', \\'eval\\']')\n",
    "    print('The number of {} data: {}'.format(mode, len(imgs)))\n",
    "\n",
    "    # Test Data Validity\n",
    "    imgs_length = len(imgs)\n",
    "\n",
    "    assert len(imgs) == len(labels), \\\n",
    "        \"length of train_imgs({}) should be the same as train_labels({})\".format(len(imgs), len(labels))\n",
    "\n",
    "    # Generate batch data\n",
    "    # define the index of every singel data\n",
    "    index_list = list(range(imgs_length))\n",
    "    # Shuffle the index of data\n",
    "    random.shuffle(index_list)\n",
    "    \n",
    "    # data generator:\n",
    "    def data_generator():\n",
    "        imgs_list = []\n",
    "        labels_list = []\n",
    "    \n",
    "        for i in index_list:\n",
    "            img = np.array(imgs[i]).astype('float32')\n",
    "            # norm_img(img)\n",
    "            label = np.array(labels[i]).astype('int64')\n",
    "            imgs_list.append(img)\n",
    "            labels_list.append(label)\n",
    "            if len(imgs_list) == BATCH_SIZE:\n",
    "                # get a dataset with length BATCHSIZE\n",
    "                yield np.array(imgs_list), np.array(labels_list)\n",
    "                # clear\n",
    "                imgs_list = []\n",
    "                labels_list = []\n",
    "    \n",
    "        if len(imgs_list) > 0:\n",
    "            yield np.array(imgs_list), np.array(labels_list)\n",
    "\n",
    "    return data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "e3caf070-1ee3-4a1b-89bd-97116f8a18f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose optimizer;\n",
    "def choose_opt(opt = 'Adam'):\n",
    "    pass\n",
    "\n",
    "# train model with cross-entropy\n",
    "def train(model, EPOCH_NUM = 10, BATCH_SIZE = 100):\n",
    "    model.train()\n",
    "    # load data\n",
    "    train_loader = load_data('train', BATCH_SIZE)\n",
    "    # choose optimizer; \n",
    "    # opt = paddle.optimizer.SGD(learning_rate = 0.01, parameters = model.parameters())\n",
    "    # opt = paddle.optimizer.Momentum(learning_rate = 0.01, momentum = 0.9, parameters = model.parameters())\n",
    "    # opt = paddle.optimizer.Adagrad(learning_rate = 0.01, parameters = model.parameters())\n",
    "    opt = paddle.optimizer.Adam(learning_rate = 0.01, parameters = model.parameters())\n",
    "    opt = paddle.optimizer.Adam(learning_rate = 0.01, weight_decay = paddle.regularizer.L2Decay(coeff=1e-2), parameters = model.parameters())\n",
    "\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            # 1. prepare data\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.to_tensor(labels)\n",
    "            # print(labels)\n",
    "            # 2. Forward calculation\n",
    "            predicts, acc = model(images, labels)\n",
    "            # print(predicts)\n",
    "            # 3. Calculate losses with cross-entropy\n",
    "            loss = F.cross_entropy(predicts, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "            # print loss every 200 batches of data\n",
    "            if batch_id % 200 == 0:\n",
    "                print(\"epoch: {} / batch: {}, loss = {}, acc = {}\".format(epoch_id, batch_id, float(avg_loss), float(acc)))\n",
    "            \n",
    "            # 4. Backpropagation\n",
    "            avg_loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "\n",
    "    # save model\n",
    "    paddle.save(model.state_dict(), params_files_path)\n",
    "    print('training is done! model has saved in \\'./{}'.format(params_files_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "4171cdb0-1f3a-42ad-ab83-03fc0cf1af74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model):\n",
    "    print('starting evaluation...')\n",
    "    param_dict = paddle.load(params_file_path)\n",
    "    model.load_dict(param_dict)\n",
    "    \n",
    "    model.eval()\n",
    "    eval_loader = load_data('eval')\n",
    "\n",
    "    acc_set = []\n",
    "    avg_loss_set = []\n",
    "    for batch_id, data in enumerate(eval_loader()):\n",
    "        # 1. prepare data\n",
    "        images, labels = data\n",
    "        images = paddle.to_tensor(images)\n",
    "        labels = paddle.to_tensor(labels)\n",
    "        # 2. Forward calculation\n",
    "        \n",
    "        predicts, acc = model(images, labels)\n",
    "        loss = F.cross_entropy(predicts, labels)\n",
    "        avg_loss = paddle.mean(loss)\n",
    "        acc_set.append(float(acc))\n",
    "        avg_loss_set.append(float(avg_loss))\n",
    "\n",
    "    # calculation accuracy\n",
    "    acc_val_mean = np.array(acc_set).mean()\n",
    "    avg_loss_val_mean = np.array(avg_loss_set).mean()\n",
    "    print('loss = {}, acc = {}'.format(avg_loss_val_mean, acc_val_mean))\n",
    "    \n",
    "    return acc_val_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "fda3738e-30b9-4f18-a9d9-9923ea953724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_path):\n",
    "    im = Image.open(img_path).convert('L')\n",
    "    im = im.resize((28,28), Image.ANTIALIAS)\n",
    "    im = np.array(im).reshape(1,1,28,28).astype(np.float32)\n",
    "    # normalize image\n",
    "    im = 1 - im * 2 / 255.0\n",
    "    # print(im)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "b65d8ace-ce1f-4ecc-a04f-27da3179d693",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNIST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "fff48ef6-6271-494e-9a27-99b1193eab09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading mnist dataset from ./work/mnist.json.gz ......\n",
      "mnist dataset load done\n",
      "The number of train data: 50000\n",
      "epoch: 0 / batch: 0, loss = 4.867584705352783, acc = 0.10000000149011612\n",
      "epoch: 0 / batch: 200, loss = 2.300097703933716, acc = 0.11999999731779099\n",
      "epoch: 0 / batch: 400, loss = 0.5449981093406677, acc = 0.8500000238418579\n",
      "epoch: 1 / batch: 0, loss = 0.23122312128543854, acc = 0.9399999976158142\n",
      "epoch: 1 / batch: 200, loss = 0.4233676791191101, acc = 0.8700000047683716\n",
      "epoch: 1 / batch: 400, loss = 0.1553206443786621, acc = 0.949999988079071\n",
      "epoch: 2 / batch: 0, loss = 0.19009146094322205, acc = 0.9399999976158142\n",
      "epoch: 2 / batch: 200, loss = 0.32649925351142883, acc = 0.8899999856948853\n",
      "epoch: 2 / batch: 400, loss = 0.15676245093345642, acc = 0.9599999785423279\n",
      "epoch: 3 / batch: 0, loss = 0.16446207463741302, acc = 0.9399999976158142\n",
      "epoch: 3 / batch: 200, loss = 0.2841870188713074, acc = 0.8999999761581421\n",
      "epoch: 3 / batch: 400, loss = 0.16269247233867645, acc = 0.949999988079071\n",
      "epoch: 4 / batch: 0, loss = 0.10774867981672287, acc = 0.9700000286102295\n",
      "epoch: 4 / batch: 200, loss = 0.2638477385044098, acc = 0.9200000166893005\n",
      "epoch: 4 / batch: 400, loss = 0.16560927033424377, acc = 0.949999988079071\n",
      "epoch: 5 / batch: 0, loss = 0.1427391767501831, acc = 0.949999988079071\n",
      "epoch: 5 / batch: 200, loss = 0.21614381670951843, acc = 0.9200000166893005\n",
      "epoch: 5 / batch: 400, loss = 0.17443427443504333, acc = 0.9399999976158142\n",
      "epoch: 6 / batch: 0, loss = 0.12347689270973206, acc = 0.9599999785423279\n",
      "epoch: 6 / batch: 200, loss = 0.24588556587696075, acc = 0.9300000071525574\n",
      "epoch: 6 / batch: 400, loss = 0.14496521651744843, acc = 0.9800000190734863\n",
      "epoch: 7 / batch: 0, loss = 0.24355854094028473, acc = 0.9100000262260437\n",
      "epoch: 7 / batch: 200, loss = 0.23736169934272766, acc = 0.9200000166893005\n",
      "epoch: 7 / batch: 400, loss = 0.16803576052188873, acc = 0.9399999976158142\n",
      "epoch: 8 / batch: 0, loss = 0.14933304488658905, acc = 0.949999988079071\n",
      "epoch: 8 / batch: 200, loss = 0.2545374631881714, acc = 0.9200000166893005\n",
      "epoch: 8 / batch: 400, loss = 0.18240343034267426, acc = 0.9300000071525574\n",
      "epoch: 9 / batch: 0, loss = 0.19964580237865448, acc = 0.9399999976158142\n",
      "epoch: 9 / batch: 200, loss = 0.21068544685840607, acc = 0.9399999976158142\n",
      "epoch: 9 / batch: 400, loss = 0.15330061316490173, acc = 0.9700000286102295\n",
      "training is done! model has saved in './mnist.pdparams\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "62f40a00-8487-47ff-a8f9-882393fb5b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting evaluation...\n",
      "loading mnist dataset from ./work/mnist.json.gz ......\n",
      "mnist dataset load done\n",
      "The number of eval data: 10000\n",
      "loss = 0.16657190918922424, acc = 0.9507000052928924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9507000052928924"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "param_dict = paddle.load(params_file_path)\n",
    "model.load_dict(param_dict)\n",
    "# load data\n",
    "model.eval()\n",
    "evaluation(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "c03054f0-b1cd-433a-810b-9aaa396cf819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction of file work/0.jpg is 0\n",
      "The prediction of file work/1.jpg is 8\n",
      "The prediction of file work/2.jpg is 2\n",
      "The prediction of file work/3.jpg is 4\n",
      "The prediction of file work/4.jpg is 4\n",
      "The prediction of file work/5.jpg is 4\n",
      "The prediction of file work/6.jpg is 6\n",
      "The prediction of file work/7.jpg is 7\n",
      "The prediction of file work/8.jpg is 2\n",
      "The prediction of file work/9.jpg is 7\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    img_path = 'work/{}.jpg'.format(i)\n",
    "    tensor_img = load_image(img_path)\n",
    "    results = model(paddle.to_tensor(tensor_img))\n",
    "    # print(results)\n",
    "    lab = np.argsort(results.numpy())\n",
    "    print('The prediction of file {} is {}'.format(img_path, lab[0][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3057805d-fd65-4cd5-b674-e46d5c68251e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
