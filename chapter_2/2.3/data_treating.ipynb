{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18a92f3a-cab6-452f-bd45-c9ee85726acc",
   "metadata": {},
   "source": [
    "# Data Treating\n",
    "\n",
    "Generally, there are five steps in data treating:\n",
    "\n",
    "1. Read data;\n",
    "2. Divide dataset;\n",
    "3. Generate batch data\n",
    "4. Shuffle dataset;\n",
    "5. Test data validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a751bcf-e4c4-4ff7-95c6-424c2de39b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddle.nn import Linear\n",
    "import paddle.nn.functional as F\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6e91bd-6e97-4fff-ad6e-67629194e072",
   "metadata": {},
   "source": [
    "## Read Data and Divide Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "081f9b3f-4f21-4b5f-b189-252722cae7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading mnist dataset from ./work/mnist.json.gz ......\n",
      "mnist dataset load done\n",
      "The number of training data:  50000\n",
      "The number of verifying data:  10000\n",
      "The number of testing data:  10000\n"
     ]
    }
   ],
   "source": [
    "datafile = './work/mnist.json.gz'\n",
    "print('loading mnist dataset from {} ......'.format(datafile))\n",
    "# load json data fiel\n",
    "data = json.load(gzip.open(datafile))\n",
    "print('mnist dataset load done')\n",
    "# unpackage the data into training set, verifying set and testing set.\n",
    "train_set, val_set, eval_set = data\n",
    "print('The number of training data: ', len(train_set[0]))\n",
    "print('The number of verifying data: ', len(val_set[0]))\n",
    "print('The number of testing data: ', len(eval_set[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c439bfd2-dc6d-4a3b-8699-a2bf70e5ff72",
   "metadata": {},
   "source": [
    "## Generate Batch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07f8136c-885d-43e0-9706-bb6d0445b050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image dimension: (100, 784), label dimension: (100,)\n"
     ]
    }
   ],
   "source": [
    "imgs, labels = train_set[0], train_set[1]\n",
    "imgs_length = len(imgs)\n",
    "# define the index of every singel data\n",
    "index_list = list(range(imgs_length))\n",
    "# Shuffle the index of data\n",
    "random.shuffle(index_list)\n",
    "\n",
    "# define batch size\n",
    "BATCHSIZE = 100\n",
    "\n",
    "# @data generator:\n",
    "def data_generator():\n",
    "    imgs_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for i in index_list:\n",
    "        img = np.array(imgs[i]).astype('float32')\n",
    "        label = np.array(labels[i]).astype('float32')\n",
    "        imgs_list.append(img)\n",
    "        labels_list.append(label)\n",
    "        if len(imgs_list) == BATCHSIZE:\n",
    "            # get a dataset with length BATCHSIZE\n",
    "            yield np.array(imgs_list), np.array(labels_list)\n",
    "            # clear\n",
    "            imgs_list = []\n",
    "            labels_list = []\n",
    "\n",
    "    if len(imgs_list) > 0:\n",
    "        yield np.array(imgs_list), np.array(labels_list)\n",
    "\n",
    "    return data_generator\n",
    "\n",
    "train_loader = data_generator\n",
    "# Read data in an iterative manner\n",
    "for batch_id, data in enumerate(train_loader()):\n",
    "    image_data, label_data = data\n",
    "    if batch_id == 0:\n",
    "        print('image dimension: {}, label dimension: {}'.format(image_data.shape, label_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cc6abc-2441-4938-a903-4923a845f896",
   "metadata": {},
   "source": [
    "## Test Data Validity\n",
    "\n",
    "There are mainly two ways to test data validity:\n",
    "\n",
    "1. Machine calibration;\n",
    "2. Manual verification.\n",
    "\n",
    "### Machine Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9598d86d-9933-47b9-a6e5-5f5d22414322",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(imgs) == len(labels), \\\n",
    "    \"length of train_imgs({}) should be the same as train_labels({})\".format(len(imgs), len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473df0b9-0467-4155-9374-c32b7bb1800e",
   "metadata": {},
   "source": [
    "## Encapsulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "994fd169-1e10-4fbd-8992-6e932ae58762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize images' data into [0, 1], and reshape them into [batch_size, w*h]\n",
    "def norm_img(img):\n",
    "    assert len(img.shape) == 3\n",
    "    batch_size, img_h, img_w = img.shape\n",
    "    img = img / 255\n",
    "    img = paddle.reshape(img, [batch_size, img_h*img_w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37a74267-8735-4a73-ba91-6e9d50ad2115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(mode = 'train'):\n",
    "    # Read Data and Divide Dataset\n",
    "    datafile = './work/mnist.json.gz'\n",
    "    print('loading mnist dataset from {} ......'.format(datafile))\n",
    "    ## load json data fiel\n",
    "    data = json.load(gzip.open(datafile))\n",
    "    print('mnist dataset load done')\n",
    "    ## unpackage the data into training set, verifying set and testing set.\n",
    "    train_set, val_set, eval_set = data\n",
    "\n",
    "    if mode == 'train':\n",
    "        imgs, labels = train_set[0], train_set[1]\n",
    "    elif mode == 'valid':\n",
    "        imgs, labels = val_set[0], val_set[1]\n",
    "    elif mode == 'eval':\n",
    "        imgs, labels = eval_set[0], eval_set[1]\n",
    "    else:\n",
    "        raise Exception('mode can only be one of [\\'train\\', \\'valid\\', \\'eval\\']')\n",
    "    print('The number of {} data: {}'.format(mode, len(imgs)))\n",
    "\n",
    "    # Test Data Validity\n",
    "    imgs_length = len(imgs)\n",
    "\n",
    "    assert len(imgs) == len(labels), \\\n",
    "        \"length of train_imgs({}) should be the same as train_labels({})\".format(len(imgs), len(labels))\n",
    "\n",
    "    # Generate batch data\n",
    "    # define the index of every singel data\n",
    "    index_list = list(range(imgs_length))\n",
    "    # Shuffle the index of data\n",
    "    random.shuffle(index_list)\n",
    "    \n",
    "    # define batch size\n",
    "    BATCH_SIZE = 100\n",
    "    \n",
    "    # @data generator:\n",
    "    def data_generator():\n",
    "        imgs_list = []\n",
    "        labels_list = []\n",
    "    \n",
    "        for i in index_list:\n",
    "            img = np.array(imgs[i]).astype('float32')\n",
    "            # norm_img(img)\n",
    "            label = np.array(labels[i]).astype('float32')\n",
    "            imgs_list.append(img)\n",
    "            labels_list.append(label)\n",
    "            if len(imgs_list) == BATCH_SIZE:\n",
    "                # get a dataset with length BATCHSIZE\n",
    "                yield np.array(imgs_list), np.array(labels_list)\n",
    "                # clear\n",
    "                imgs_list = []\n",
    "                labels_list = []\n",
    "    \n",
    "        if len(imgs_list) > 0:\n",
    "            yield np.array(imgs_list), np.array(labels_list)\n",
    "\n",
    "    return data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93941d83-aa88-434e-b4ff-3824561589c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net structure of MNIST\n",
    "class MNIST(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(MNIST, self).__init__()\n",
    "\n",
    "        # define one fully connected layer, \n",
    "        # which has 13 input dimensions and 1 output dimensions\n",
    "        self.fc = Linear(in_features = 784, out_features = 1)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x = self.fc(inputs)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8475e54e-e28c-457a-ae66-fa71e655237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "def train(model):\n",
    "    model.train()\n",
    "    # load data\n",
    "    train_loader = load_data('train')\n",
    "    opt = paddle.optimizer.SGD(learning_rate = 0.001, parameters = model.parameters())\n",
    "    \n",
    "    EPOCH_NUM = 10\n",
    "\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            # 1. prepare data\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.to_tensor(labels)\n",
    "            # 2. Forward calculation\n",
    "            predicts = model(images)\n",
    "            # 3. Calculate losses\n",
    "            loss = F.square_error_cost(predicts, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "            # print loss every 200 batches of data\n",
    "            if batch_id % 200 == 0:\n",
    "                print(\"epoch: {} / batch: {}, loss = {}\".format(epoch_id, batch_id, float(avg_loss)))\n",
    "            \n",
    "            # 4. Backpropagation\n",
    "            avg_loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "\n",
    "    # save model\n",
    "    paddle.save(model.state_dict(), './mnist.pdparams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14661dd7-7d9f-4572-a56a-d456f27e574a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading mnist dataset from ./work/mnist.json.gz ......\n",
      "mnist dataset load done\n",
      "The number of train data: 50000\n",
      "epoch: 0 / batch: 0, loss = 20.97543716430664\n",
      "epoch: 0 / batch: 200, loss = 7.980806827545166\n",
      "epoch: 0 / batch: 400, loss = 8.513910293579102\n",
      "epoch: 1 / batch: 0, loss = 8.79451847076416\n",
      "epoch: 1 / batch: 200, loss = 7.934875011444092\n",
      "epoch: 1 / batch: 400, loss = 8.479248046875\n",
      "epoch: 2 / batch: 0, loss = 8.771469116210938\n",
      "epoch: 2 / batch: 200, loss = 7.923612594604492\n",
      "epoch: 2 / batch: 400, loss = 8.465299606323242\n",
      "epoch: 3 / batch: 0, loss = 8.762093544006348\n",
      "epoch: 3 / batch: 200, loss = 7.918120384216309\n",
      "epoch: 3 / batch: 400, loss = 8.457737922668457\n",
      "epoch: 4 / batch: 0, loss = 8.756901741027832\n",
      "epoch: 4 / batch: 200, loss = 7.914795398712158\n",
      "epoch: 4 / batch: 400, loss = 8.452914237976074\n",
      "epoch: 5 / batch: 0, loss = 8.753449440002441\n",
      "epoch: 5 / batch: 200, loss = 7.912471294403076\n",
      "epoch: 5 / batch: 400, loss = 8.449481964111328\n",
      "epoch: 6 / batch: 0, loss = 8.750893592834473\n",
      "epoch: 6 / batch: 200, loss = 7.910694599151611\n",
      "epoch: 6 / batch: 400, loss = 8.446884155273438\n",
      "epoch: 7 / batch: 0, loss = 8.748851776123047\n",
      "epoch: 7 / batch: 200, loss = 7.909256935119629\n",
      "epoch: 7 / batch: 400, loss = 8.44483757019043\n",
      "epoch: 8 / batch: 0, loss = 8.747172355651855\n",
      "epoch: 8 / batch: 200, loss = 7.908023357391357\n",
      "epoch: 8 / batch: 400, loss = 8.443151473999023\n",
      "epoch: 9 / batch: 0, loss = 8.74575138092041\n",
      "epoch: 9 / batch: 200, loss = 7.906978130340576\n",
      "epoch: 9 / batch: 400, loss = 8.441746711730957\n"
     ]
    }
   ],
   "source": [
    "model = MNIST()\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bec4bb-ef71-4e19-a8c4-a49d3c8ac233",
   "metadata": {},
   "source": [
    "## Asynchronous Data Treating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18341bd4-3059-4990-864c-729ee33229d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
