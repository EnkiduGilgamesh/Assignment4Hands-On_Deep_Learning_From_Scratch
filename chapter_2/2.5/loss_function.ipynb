{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19ff1fab-93ad-43f5-be49-2142eea4b9ca",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "The Boston house price prediction task in the first chapter is a regression task whose result is a continuous value, so we can use mean squared error as a loss function. However, the result of the classification task is discrete labeling, so it is unreasonable to use mean squared error as a loss function.\n",
    "\n",
    "## Softmax Function\n",
    "\n",
    "$$\n",
    "Softmax(x_i) = \\frac{e^{x_i}}{\\sum\\limits_{j=0}^{N}{e^{x_j}}}\n",
    "$$\n",
    "\n",
    "As can be seen from the formula, the range of each output is between 0~1, and the sum of all outputs is equal to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b71a9d-29f7-4dfe-a89b-d1ef2756e9b1",
   "metadata": {},
   "source": [
    "## Cross-entropy\n",
    "\n",
    "People are used to using cross-entropy as a loss function for classification problems.\n",
    "\n",
    "Firstly, \"one-bit valid encoding\" for categories:\n",
    "\n",
    "$$\n",
    "\\begin{array}\n",
    "\\mathbf{y} =& [y_1, y_2, \\dots, y_n]^T \\\\\n",
    "y_i =& \\left\\{ \\begin{array}\\\n",
    "    & 1\\ & \\mathrm{if}\\ i = y \\\\\n",
    "    & 0\\ & \\mathrm{otherwise}\n",
    "    \\end{array} \\right.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The maximum value is the forecast:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\mathop{\\mathrm{argmax}}\\limits_i o_i\n",
    "$$\n",
    "\n",
    "Cross-entropy is often used to measure the difference between two probabilities:\n",
    "\n",
    "$$\n",
    "H(\\mathbf{p},\\mathbf{q}) = -\\sum\\limits_i{p_i\\log{q_i}}\n",
    "$$\n",
    "\n",
    "Use it as loss function:\n",
    "\n",
    "$$\n",
    "l(y, \\hat{y}) = -\\sum\\limits_i y_i\\log{\\hat{y}_i} = -\\log{\\hat{y}_y}\n",
    "$$\n",
    "\n",
    "Its gradient is the difference between true probability and predicted probability:\n",
    "\n",
    "$$\n",
    "\\partial_{o_i} l(y, \\hat{y}) = \\mathrm{softmax}(\\mathbf{o})_i - y_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32e5fd14-6af7-4723-bbbc-2ba26fca8fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddle.nn import Conv2D, MaxPool2D, Linear\n",
    "from PIL import Image\n",
    "import paddle.nn.functional as F\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45c9e050-7682-4514-966c-5dc15c7dee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(mode = 'train'):\n",
    "    # Read Data and Divide Dataset\n",
    "    datafile = './work/mnist.json.gz'\n",
    "    print('loading mnist dataset from {} ......'.format(datafile))\n",
    "    ## load json data fiel\n",
    "    data = json.load(gzip.open(datafile))\n",
    "    print('mnist dataset load done')\n",
    "    ## unpackage the data into training set, verifying set and testing set.\n",
    "    train_set, val_set, eval_set = data\n",
    "\n",
    "    if mode == 'train':\n",
    "        imgs, labels = train_set[0], train_set[1]\n",
    "    elif mode == 'valid':\n",
    "        imgs, labels = val_set[0], val_set[1]\n",
    "    elif mode == 'eval':\n",
    "        imgs, labels = eval_set[0], eval_set[1]\n",
    "    else:\n",
    "        raise Exception('mode can only be one of [\\'train\\', \\'valid\\', \\'eval\\']')\n",
    "    print('The number of {} data: {}'.format(mode, len(imgs)))\n",
    "\n",
    "    # Test Data Validity\n",
    "    imgs_length = len(imgs)\n",
    "\n",
    "    assert len(imgs) == len(labels), \\\n",
    "        \"length of train_imgs({}) should be the same as train_labels({})\".format(len(imgs), len(labels))\n",
    "\n",
    "    # Generate batch data\n",
    "    # define the index of every singel data\n",
    "    index_list = list(range(imgs_length))\n",
    "    # Shuffle the index of data\n",
    "    random.shuffle(index_list)\n",
    "    \n",
    "    # define batch size\n",
    "    BATCH_SIZE = 100\n",
    "    \n",
    "    # @data generator:\n",
    "    def data_generator():\n",
    "        imgs_list = []\n",
    "        labels_list = []\n",
    "    \n",
    "        for i in index_list:\n",
    "            img = np.array(imgs[i]).astype('float32')\n",
    "            # norm_img(img)\n",
    "            label = np.array(labels[i]).astype('int64')\n",
    "            imgs_list.append(img)\n",
    "            labels_list.append(label)\n",
    "            if len(imgs_list) == BATCH_SIZE:\n",
    "                # get a dataset with length BATCHSIZE\n",
    "                yield np.array(imgs_list), np.array(labels_list)\n",
    "                # clear\n",
    "                imgs_list = []\n",
    "                labels_list = []\n",
    "    \n",
    "        if len(imgs_list) > 0:\n",
    "            yield np.array(imgs_list), np.array(labels_list)\n",
    "\n",
    "    return data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3caf070-1ee3-4a1b-89bd-97116f8a18f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model with cross-entropy\n",
    "def train(model):\n",
    "    model.train()\n",
    "    # load data\n",
    "    train_loader = load_data('train')\n",
    "    opt = paddle.optimizer.SGD(learning_rate = 0.001, parameters = model.parameters())\n",
    "    \n",
    "    EPOCH_NUM = 10\n",
    "\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            # 1. prepare data\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.to_tensor(labels)\n",
    "            # 2. Forward calculation\n",
    "            predicts = model(images)\n",
    "            # 3. Calculate losses with cross-entropy\n",
    "            loss = F.cross_entropy(predicts, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "            # print loss every 200 batches of data\n",
    "            if batch_id % 200 == 0:\n",
    "                print(\"epoch: {} / batch: {}, loss = {}\".format(epoch_id, batch_id, float(avg_loss)))\n",
    "            \n",
    "            # 4. Backpropagation\n",
    "            avg_loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "\n",
    "    # save model\n",
    "    paddle.save(model.state_dict(), './mnist.pdparams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4171cdb0-1f3a-42ad-ab83-03fc0cf1af74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, datasets):\n",
    "    model.eval()\n",
    "\n",
    "    acc_set = list()\n",
    "    for batch_id, data in enumerate(datasets()):\n",
    "        # 1. prepare data\n",
    "        images, labels = data\n",
    "        images = paddle.to_tensor(images)\n",
    "        labels = paddle.to_tensor(labels)\n",
    "        # 2. Forward calculation\n",
    "        pred = model(images)\n",
    "\n",
    "        acc = paddle.metric.accuracy(input = pred, label = labels)\n",
    "        acc_set.extend(acc.numpy())\n",
    "\n",
    "    # calculation accuracy\n",
    "    acc_val_mean = np.array(acc_set).mean()\n",
    "    return acc_val_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d04d5903-7bdc-4642-9437-2f2dd45f4431",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(MNIST, self).__init__()\n",
    "\n",
    "        # Define the convolutional layer 1 with \n",
    "        # the output feature channel (out_channels) set to 20, \n",
    "        # the convolution kernel size (kernel_size) to 5, \n",
    "        # the convolution step size stride to 1, and padding to 2\n",
    "        self.conv1 = Conv2D(in_channels = 1, out_channels = 20, kernel_size = 5, stride = 1, padding = 2)\n",
    "        # Define the pooling layer 1 with  \n",
    "        # the size of the pooling kernel (kernel_size) to 2, \n",
    "        # the pooling step to 2\n",
    "        self.max_pool1 = MaxPool2D(kernel_size = 2, stride = 2)\n",
    "        # Define the convolutional layer 2\n",
    "        self.conv2 = Conv2D(in_channels = 20, out_channels = 20, kernel_size = 5, stride = 1, padding = 2)\n",
    "        # Define the pooling layer 2\n",
    "        self.max_pool2 = MaxPool2D(kernel_size = 2, stride = 2)\n",
    "        # Define a fully connected layer\n",
    "        self.fc = Linear(in_features = 980, out_features = 10)\n",
    "\n",
    "    # define farword calculation function, the activation function of hidden layers is ReLU\n",
    "    def forward(self, inputs):\n",
    "        inputs = paddle.reshape(inputs, [inputs.shape[0], 1, 28, 28])\n",
    "        x = self.conv1(inputs)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool2(x)\n",
    "        x = paddle.reshape(x, [x.shape[0], -1])\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fda3738e-30b9-4f18-a9d9-9923ea953724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_path):\n",
    "    im = Image.open(img_path).convert('L')\n",
    "    im = im.resize((28,28), Image.ANTIALIAS)\n",
    "    im = np.array(im).reshape(1,1,28,28).astype(np.float32)\n",
    "    # normalize image\n",
    "    im = 1.0 - im / 255.0\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b65d8ace-ce1f-4ecc-a04f-27da3179d693",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNIST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fff48ef6-6271-494e-9a27-99b1193eab09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading mnist dataset from ./work/mnist.json.gz ......\n",
      "mnist dataset load done\n",
      "The number of train data: 50000\n",
      "epoch: 0 / batch: 0, loss = 3.6161539554595947\n",
      "epoch: 0 / batch: 200, loss = 1.2341917753219604\n",
      "epoch: 0 / batch: 400, loss = 0.8244312405586243\n",
      "epoch: 1 / batch: 0, loss = 0.7366979718208313\n",
      "epoch: 1 / batch: 200, loss = 0.447758287191391\n",
      "epoch: 1 / batch: 400, loss = 0.508483350276947\n",
      "epoch: 2 / batch: 0, loss = 0.4283043146133423\n",
      "epoch: 2 / batch: 200, loss = 0.29235202074050903\n",
      "epoch: 2 / batch: 400, loss = 0.4047854542732239\n",
      "epoch: 3 / batch: 0, loss = 0.31942665576934814\n",
      "epoch: 3 / batch: 200, loss = 0.23105426132678986\n",
      "epoch: 3 / batch: 400, loss = 0.3476784825325012\n",
      "epoch: 4 / batch: 0, loss = 0.2622949182987213\n",
      "epoch: 4 / batch: 200, loss = 0.1972339004278183\n",
      "epoch: 4 / batch: 400, loss = 0.30950772762298584\n",
      "epoch: 5 / batch: 0, loss = 0.22579124569892883\n",
      "epoch: 5 / batch: 200, loss = 0.17410260438919067\n",
      "epoch: 5 / batch: 400, loss = 0.2815054953098297\n",
      "epoch: 6 / batch: 0, loss = 0.20008377730846405\n",
      "epoch: 6 / batch: 200, loss = 0.15693318843841553\n",
      "epoch: 6 / batch: 400, loss = 0.25953811407089233\n",
      "epoch: 7 / batch: 0, loss = 0.18049459159374237\n",
      "epoch: 7 / batch: 200, loss = 0.14354893565177917\n",
      "epoch: 7 / batch: 400, loss = 0.24164935946464539\n",
      "epoch: 8 / batch: 0, loss = 0.16473186016082764\n",
      "epoch: 8 / batch: 200, loss = 0.13263161480426788\n",
      "epoch: 8 / batch: 400, loss = 0.22622865438461304\n",
      "epoch: 9 / batch: 0, loss = 0.1518772840499878\n",
      "epoch: 9 / batch: 200, loss = 0.1233612447977066\n",
      "epoch: 9 / batch: 400, loss = 0.21329742670059204\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62f40a00-8487-47ff-a8f9-882393fb5b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of this prediction is  [7 0 9 4 1 2 6 5 3 8]\n"
     ]
    }
   ],
   "source": [
    "params_file_path = 'mnist.pdparams'\n",
    "img_path = 'work/9.jpg'\n",
    "# load model\n",
    "param_dict = paddle.load(params_file_path)\n",
    "model.load_dict(param_dict)\n",
    "# load data\n",
    "model.eval()\n",
    "tensor_img = load_image(img_path)\n",
    "results = model(paddle.to_tensor(tensor_img))\n",
    "lab = np.argsort(results.numpy())\n",
    "print('The result of this prediction is ', lab[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03054f0-b1cd-433a-810b-9aaa396cf819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3057805d-fd65-4cd5-b674-e46d5c68251e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
